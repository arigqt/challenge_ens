{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage.segmentation import watershed, felzenszwalb\n",
    "from skimage.filters import sobel\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import rank\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.morphology import disk\n",
    "import sklearn.metrics\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "import zipfile\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_data, plot_slice_seg, rand_index_dataset, prediction_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### X_train collected ####\n",
      "#### X_test collected ####\n",
      "#### y_train collected ####\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Model : W-Net architecture\n",
    "see this article : https://arxiv.org/pdf/1711.08506.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](https://miro.medium.com/max/1029/1*FU2BbaWCShWLvf6QsNGXlA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every block is composed of convolution/batch normalization/relu -> convolution/batch normalization/relu. There's a pooling layer between every block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, seperable=True):\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        if seperable:\n",
    "            \n",
    "# ---- We define here the 2 depthwise seperable convolutions inside a separable block\n",
    "               \n",
    "            self.depth_conv1 = nn.Conv2d(in_filters, in_filters, kernel_size=3, groups=in_filters, padding=1)\n",
    "            self.point_conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=1)\n",
    "            self.conv1 = nn.Sequential(self.depth_conv1, self.point_conv1)\n",
    "            \n",
    "            self.depth_conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, groups=out_filters, padding=1)\n",
    "            self.point_conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=1)\n",
    "            self.conv2 = nn.Sequential(self.depth_conv2, self.point_conv2)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "# ---- We define here the 2 convolutions inside a standard block\n",
    "            \n",
    "            self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1)\n",
    "    \n",
    "        self.batchnorm1 = nn.BatchNorm2d(out_filters)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(out_filters)\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "# ---- We define here a block : convolution/batch norm/relu, convolution/batch norm/relu\n",
    "        \n",
    "        x = self.batchnorm1(self.conv1(x)).clamp(0)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    " \n",
    "        x = self.batchnorm2(self.conv2(x)).clamp(0)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_channels=3, filters=64):\n",
    "\n",
    "        super(UEnc, self).__init__()\n",
    "        \n",
    "        self.enc1 = Block(in_channels, filters, seperable=False)\n",
    "        self.enc2 = Block(filters, 2*filters)\n",
    "        self.enc3 = Block(2*filters, 4*filters)\n",
    "        self.enc4 = Block(4*filters, 8*filters)   \n",
    "        self.enc5 = Block(8*filters, 16*filters) #minimal dimension space\n",
    "        \n",
    "\n",
    "# ---- We define the transpose convolution, that does up pooling 2x2 (inverse of maxpooling)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(16*filters, 8*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec1 = Block(16*filters, 8*filters)\n",
    "        self.up2 = nn.ConvTranspose2d(8*filters, 4*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec2 = Block(8*filters, 4*filters)\n",
    "        self.up3 = nn.ConvTranspose2d(4*filters, 2*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = Block(4*filters, 2*filters)\n",
    "        self.up4 = nn.ConvTranspose2d(2*filters, filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = Block(2*filters, filters, seperable=False)\n",
    "        \n",
    "        self.final = nn.Conv2d(filters, k, kernel_size=(1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "# ---- We define the down sampling phase : block/max pooling\n",
    "\n",
    "        enc1 = self.enc1(x)\n",
    "        maxpool1 = F.max_pool2d(enc1, (2, 2))   \n",
    "        enc2 = self.enc2(maxpool1)\n",
    "        maxpool2 = F.max_pool2d(enc2, (2, 2))    \n",
    "        enc3 = self.enc3(maxpool2)\n",
    "        maxpool3 = F.max_pool2d(enc3, (2, 2))    \n",
    "        enc4 = self.enc4(maxpool3)\n",
    "        maxpool4 = F.max_pool2d(enc4, (2, 2))      \n",
    "        enc5=self.enc5(maxpool4)   \n",
    "        \n",
    "# ---- We define the up sampling phase : transposed convolution(= up pooling)/block\n",
    "# ---- We define the skip connections\n",
    "\n",
    "        up1 = torch.cat([enc4, self.up1(enc5)], 1) # skip connection : concatenate previous layer output + enc4 output\n",
    "        dec1 = self.dec1(up1)   \n",
    "        up2 = torch.cat([enc3, self.up2(dec1)], 1)\n",
    "        dec2 = self.dec2(up2)   \n",
    "        up3 = torch.cat([enc2, self.up3(dec2)], 1)\n",
    "        dec3 = self.dec3(up3)  \n",
    "        up4 = torch.cat([enc1, self.up4(dec3)], 1)\n",
    "        dec4 = self.dec4(up4)       \n",
    "        final = self.final(dec4)\n",
    "            \n",
    "        return final\n",
    "\n",
    "class UDec(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_channels=3, filters=64):\n",
    "\n",
    "        super(UDec, self).__init__()\n",
    "        \n",
    "        self.enc1 = Block(k, filters, seperable=False)\n",
    "        self.enc2 = Block(filters, 2*filters)\n",
    "        self.enc3 = Block(2*filters, 4*filters)\n",
    "        self.enc4 = Block(4*filters, 8*filters)   \n",
    "        self.enc5 = Block(8*filters, 16*filters) #minimal dimension space\n",
    "        \n",
    "\n",
    "# ---- We define the transpose convolution, that does up pooling 2x2 (inverse of maxpooling)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(16*filters, 8*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec1 = Block(16*filters, 8*filters)\n",
    "        self.up2 = nn.ConvTranspose2d(8*filters, 4*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec2 = Block(8*filters, 4*filters)\n",
    "        self.up3 = nn.ConvTranspose2d(4*filters, 2*filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = Block(4*filters, 2*filters)\n",
    "        self.up4 = nn.ConvTranspose2d(2*filters, filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = Block(2*filters, filters, seperable=False)\n",
    "        \n",
    "        self.final = nn.Conv2d(filters, in_channels, kernel_size=(1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "# ---- We define the down sampling phase : block/max pooling\n",
    "\n",
    "        enc1 = self.enc1(x)\n",
    "        maxpool1 = F.max_pool2d(enc1, (2, 2))  \n",
    "        enc2 = self.enc2(maxpool1)\n",
    "        maxpool2 = F.max_pool2d(enc2, (2, 2))  \n",
    "        enc3 = self.enc3(maxpool2)\n",
    "        maxpool3 = F.max_pool2d(enc3, (2, 2))\n",
    "        enc4 = self.enc4(maxpool3)\n",
    "        maxpool4 = F.max_pool2d(enc4, (2, 2)) \n",
    "        enc5=self.enc5(maxpool4)   \n",
    "        \n",
    "# ---- We define the up sampling phase : transposed convolution(= up pooling)/block\n",
    "# ---- We define the skip connections\n",
    "\n",
    "        up1 = torch.cat([enc4, self.up1(enc5)], 1) # skip connection : concatenate previous layer output + enc4 output\n",
    "        dec1 = self.dec1(up1)  \n",
    "        up2 = torch.cat([enc3, self.up2(dec1)], 1)\n",
    "        dec2 = self.dec2(up2)  \n",
    "        up3 = torch.cat([enc2, self.up3(dec2)], 1)\n",
    "        dec3 = self.dec3(up3)\n",
    "        up4 = torch.cat([enc1, self.up4(dec3)], 1)\n",
    "        dec4 = self.dec4(up4)     \n",
    "        final = self.final(dec4)\n",
    "            \n",
    "        return final\n",
    "\n",
    "class WNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, k, filters=64, in_channels=3, out_chans=3):\n",
    "        \n",
    "        super(WNet, self).__init__()\n",
    "\n",
    "        self.UEnc = UEnc(k, in_channels, filters)\n",
    "        self.UDec = UDec(k, in_channels, filters)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        enc = self.UEnc(x)      \n",
    "        dec = self.UDec(F.softmax(enc, 1))\n",
    "\n",
    "        return enc, dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = Block(3, 3)\n",
    "enc1 = UEnc(10)\n",
    "dec1 = UDec(10)\n",
    "WNet1 = WNet(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have to create the loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert our datasets with pytorch trainloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(X_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(X_test, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "15fd02e5c878be06348de4aa47e758028f61ec77ef5ce2a754b2cbd26a6e5956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
